yarn.ipc.client.factory.class**Factory to create client IPC classes.**N/A
yarn.ipc.server.factory.class**Factory to create server IPC classes.**N/A
yarn.ipc.record.factory.class**Factory to create serializeable records.**N/A
yarn.ipc.rpc.class**RPC class implementation**org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
yarn.resourcemanager.hostname**The hostname of the RM.**0.0.0.0
yarn.resourcemanager.address**The address of the applications manager interface in the RM.**${yarn.resourcemanager.hostname}:8032
yarn.resourcemanager.bind-host**       The actual address the server will bind to. If this optional address is       set, the RPC and webapp servers will bind to this address and the port specified in       yarn.resourcemanager.address and yarn.resourcemanager.webapp.address, respectively. This       is most useful for making RM listen to all interfaces by setting to 0.0.0.0.     **N/A
yarn.resourcemanager.client.thread-count**The number of threads used to handle applications manager requests.**50
yarn.resourcemanager.amlauncher.thread-count**Number of threads used to launch/cleanup AM.**50
yarn.resourcemanager.nodemanager-connect-retries**Retry times to connect with NM.**10
yarn.dispatcher.drain-events.timeout**Timeout in milliseconds when YARN dispatcher tries to drain the       events. Typically, this happens when service is stopping. e.g. RM drains       the ATS events dispatcher when stopping.     **300000
yarn.am.liveness-monitor.expiry-interval-ms**The expiry interval for application master reporting.**600000
yarn.resourcemanager.principal**The Kerberos principal for the resource manager.**N/A
yarn.resourcemanager.scheduler.address**The address of the scheduler interface.**${yarn.resourcemanager.hostname}:8030
yarn.resourcemanager.scheduler.client.thread-count**Number of threads to handle scheduler interface.**50
yarn.http.policy**         This configures the HTTP endpoint for Yarn Daemons.The following         values are supported:         - HTTP_ONLY : Service is provided only on http         - HTTPS_ONLY : Service is provided only on https       **HTTP_ONLY
yarn.resourcemanager.webapp.address**The http address of the RM web application.**${yarn.resourcemanager.hostname}:8088
yarn.resourcemanager.webapp.https.address**The https adddress of the RM web application.**${yarn.resourcemanager.hostname}:8090
yarn.resourcemanager.resource-tracker.address**N/A**${yarn.resourcemanager.hostname}:8031
yarn.acl.enable**Are acls enabled.**false
yarn.admin.acl**ACL of who can be admin of the YARN cluster.***
yarn.resourcemanager.admin.address**The address of the RM admin interface.**${yarn.resourcemanager.hostname}:8033
yarn.resourcemanager.admin.client.thread-count**Number of threads used to handle RM admin interface.**1
yarn.resourcemanager.connect.max-wait.ms**Maximum time to wait to establish connection to     ResourceManager.**900000
yarn.resourcemanager.connect.retry-interval.ms**How often to try connecting to the     ResourceManager.**30000
yarn.resourcemanager.am.max-attempts**The maximum number of application attempts. It's a global     setting for all application masters. Each application master can specify     its individual maximum number of application attempts via the API, but the     individual number cannot be more than the global upper bound. If it is,     the resourcemanager will override it. The default number is set to 2, to     allow at least one retry for AM.**2
yarn.resourcemanager.container.liveness-monitor.interval-ms**How often to check that containers are still alive. **600000
yarn.resourcemanager.keytab**The keytab for the resource manager.**/etc/krb5.keytab
yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled**Flag to enable override of the default kerberos authentication     filter with the RM authentication filter to allow authentication using     delegation tokens(fallback to kerberos if the tokens are missing). Only     applicable when the http authentication type is kerberos.**true
yarn.resourcemanager.webapp.cross-origin.enabled**Flag to enable cross-origin (CORS) support in the RM. This flag     requires the CORS filter initializer to be added to the filter initializers     list in core-site.xml.**false
yarn.nm.liveness-monitor.expiry-interval-ms**How long to wait until a node manager is considered dead.**600000
yarn.resourcemanager.nodes.include-path**Path to file with nodes to include.**N/A
yarn.resourcemanager.nodes.exclude-path**Path to file with nodes to exclude.**N/A
yarn.resourcemanager.resource-tracker.client.thread-count**Number of threads to handle resource tracker calls.**50
yarn.resourcemanager.scheduler.class**The class to use as the resource scheduler.**org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
yarn.scheduler.minimum-allocation-mb**The minimum allocation for every container request at the RM,     in MBs. Memory requests lower than this will throw a     InvalidResourceRequestException.**1024
yarn.scheduler.maximum-allocation-mb**The maximum allocation for every container request at the RM,     in MBs. Memory requests higher than this will throw a     InvalidResourceRequestException.**8192
yarn.scheduler.minimum-allocation-vcores**The minimum allocation for every container request at the RM,     in terms of virtual CPU cores. Requests lower than this will throw a     InvalidResourceRequestException.**1
yarn.scheduler.maximum-allocation-vcores**The maximum allocation for every container request at the RM,     in terms of virtual CPU cores. Requests higher than this will throw a     InvalidResourceRequestException.**32
yarn.resourcemanager.recovery.enabled**Enable RM to recover state after starting. If true, then        yarn.resourcemanager.store.class must be specified. **false
yarn.resourcemanager.fail-fast**Should RM fail fast if it encounters any errors. By defalt, it       points to ${yarn.fail-fast}. Errors include:       1) exceptions when state-store write/read operations fails.     **${yarn.fail-fast}
yarn.fail-fast**Should YARN fail fast if it encounters any errors.       This is a global config for all other components including RM,NM etc.       If no value is set for component-specific config (e.g yarn.resourcemanager.fail-fast),       this value will be the default.     **false
yarn.resourcemanager.work-preserving-recovery.enabled**Enable RM work preserving recovery. This configuration is private     to YARN for experimenting the feature.     **true
yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms**Set the amount of time RM waits before allocating new     containers on work-preserving-recovery. Such wait period gives RM a chance     to settle down resyncing with NMs in the cluster on recovery, before assigning     new containers to applications.     **10000
yarn.resourcemanager.store.class**The class to use as the persistent store.        If org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore       is used, the store is implicitly fenced; meaning a single ResourceManager       is able to use the store at any point in time. More details on this       implicit fencing, along with setting up appropriate ACLs is discussed       under yarn.resourcemanager.zk-state-store.root-node.acl.     **org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
yarn.resourcemanager.state-store.max-completed-applications**The maximum number of completed applications RM state     store keeps, less than or equals to ${yarn.resourcemanager.max-completed-applications}.     By default, it equals to ${yarn.resourcemanager.max-completed-applications}.     This ensures that the applications kept in the state store are consistent with     the applications remembered in RM memory.     Any values larger than ${yarn.resourcemanager.max-completed-applications} will     be reset to ${yarn.resourcemanager.max-completed-applications}.     Note that this value impacts the RM recovery performance.Typically,     a smaller value indicates better performance on RM recovery.     **${yarn.resourcemanager.max-completed-applications}
yarn.resourcemanager.zk-address**Host:Port of the ZooKeeper server to be used by the RM. This       must be supplied when using the ZooKeeper based implementation of the       RM state store and/or embedded automatic failover in a HA setting.     **N/A
yarn.resourcemanager.zk-num-retries**Number of times RM tries to connect to ZooKeeper.**1000
yarn.resourcemanager.zk-retry-interval-ms**Retry interval in milliseconds when connecting to ZooKeeper.       When HA is enabled, the value here is NOT used. It is generated       automatically from yarn.resourcemanager.zk-timeout-ms and       yarn.resourcemanager.zk-num-retries.     **1000
yarn.resourcemanager.zk-state-store.parent-path**Full path of the ZooKeeper znode where RM state will be     stored. This must be supplied when using     org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore     as the value for yarn.resourcemanager.store.class**/rmstore
yarn.resourcemanager.zk-timeout-ms**ZooKeeper session timeout in milliseconds. Session expiration     is managed by the ZooKeeper cluster itself, not by the client. This value is     used by the cluster to determine when the client's session expires.     Expirations happens when the cluster does not hear from the client within     the specified session timeout period (i.e. no heartbeat).**10000
yarn.resourcemanager.zk-acl**ACL's to be used for ZooKeeper znodes.**world:anyone:rwcda
yarn.resourcemanager.zk-state-store.root-node.acl**       ACLs to be used for the root znode when using ZKRMStateStore in a HA       scenario for fencing.        ZKRMStateStore supports implicit fencing to allow a single       ResourceManager write-access to the store. For fencing, the       ResourceManagers in the cluster share read-write-admin privileges on the       root node, but the Active ResourceManager claims exclusive create-delete       permissions.        By default, when this property is not set, we use the ACLs from       yarn.resourcemanager.zk-acl for shared admin access and       rm-address:random-number for username-based exclusive create-delete       access.        This property allows users to set ACLs of their choice instead of using       the default mechanism. For fencing to work, the ACLs should be       carefully set differently on each ResourceManger such that all the       ResourceManagers have shared admin access and the Active ResourceManger       takes over (exclusively) the create-delete access.     **N/A
yarn.resourcemanager.zk-auth**         Specify the auths to be used for the ACL's specified in both the         yarn.resourcemanager.zk-acl and         yarn.resourcemanager.zk-state-store.root-node.acl properties.  This         takes a comma-separated list of authentication mechanisms, each of the         form 'scheme:auth' (the same syntax used for the 'addAuth' command in         the ZK CLI).     **N/A
yarn.resourcemanager.fs.state-store.uri**URI pointing to the location of the FileSystem path where     RM state will be stored. This must be supplied when using     org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore     as the value for yarn.resourcemanager.store.class**${hadoop.tmp.dir}/yarn/system/rmstore
yarn.resourcemanager.fs.state-store.retry-policy-spec**hdfs client retry policy specification. hdfs client retry     is always enabled. Specified in pairs of sleep-time and number-of-retries     and (t0, n0), (t1, n1), ..., the first n0 retries sleep t0 milliseconds on     average, the following n1 retries sleep t1 milliseconds on average, and so on.     **2000, 500
yarn.resourcemanager.fs.state-store.num-retries**the number of retries to recover from IOException in     FileSystemRMStateStore.     **0
yarn.resourcemanager.fs.state-store.retry-interval-ms**Retry interval in milliseconds in FileSystemRMStateStore.     **1000
yarn.resourcemanager.leveldb-state-store.path**Local path where the RM state will be stored when using     org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore     as the value for yarn.resourcemanager.store.class**${hadoop.tmp.dir}/yarn/system/rmstore
yarn.resourcemanager.leveldb-state-store.compaction-interval-secs**The time in seconds between full compactions of the leveldb     database. Setting the interval to zero disables the full compaction     cycles.**3600
yarn.resourcemanager.ha.enabled**Enable RM high-availability. When enabled,       (1) The RM starts in the Standby mode by default, and transitions to       the Active mode when prompted to.       (2) The nodes in the RM ensemble are listed in       yarn.resourcemanager.ha.rm-ids       (3) The id of each RM either comes from yarn.resourcemanager.ha.id       if yarn.resourcemanager.ha.id is explicitly specified or can be       figured out by matching yarn.resourcemanager.address.{id} with local address       (4) The actual physical addresses come from the configs of the pattern       - {rpc-config}.{id}**false
yarn.resourcemanager.ha.automatic-failover.enabled**Enable automatic failover.       By default, it is enabled only when HA is enabled**true
yarn.resourcemanager.ha.automatic-failover.embedded**Enable embedded automatic failover.       By default, it is enabled only when HA is enabled.       The embedded elector relies on the RM state store to handle fencing,       and is primarily intended to be used in conjunction with ZKRMStateStore.     **true
yarn.resourcemanager.ha.automatic-failover.zk-base-path**The base znode path to use for storing leader information,       when using ZooKeeper based leader election.**/yarn-leader-election
yarn.resourcemanager.cluster-id**Name of the cluster. In a HA setting,       this is used to ensure the RM participates in leader       election for this cluster and ensures it does not affect       other clusters**N/A
yarn.resourcemanager.ha.rm-ids**The list of RM nodes in the cluster when HA is       enabled. See description of yarn.resourcemanager.ha       .enabled for full details on how this is used.**N/A
yarn.resourcemanager.ha.id**The id (string) of the current RM. When HA is enabled, this       is an optional config. The id of current RM can be set by explicitly       specifying yarn.resourcemanager.ha.id or figured out by matching       yarn.resourcemanager.address.{id} with local address       See description of yarn.resourcemanager.ha.enabled       for full details on how this is used.**N/A
yarn.client.failover-proxy-provider**When HA is enabled, the class to be used by Clients, AMs and       NMs to failover to the Active RM. It should extend       org.apache.hadoop.yarn.client.RMFailoverProxyProvider**org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider
yarn.client.failover-max-attempts**When HA is enabled, the max number of times       FailoverProxyProvider should attempt failover. When set,       this overrides the yarn.resourcemanager.connect.max-wait.ms. When       not set, this is inferred from       yarn.resourcemanager.connect.max-wait.ms.**N/A
yarn.client.failover-sleep-base-ms**When HA is enabled, the sleep base (in milliseconds) to be       used for calculating the exponential delay between failovers. When set,       this overrides the yarn.resourcemanager.connect.* settings. When       not set, yarn.resourcemanager.connect.retry-interval.ms is used instead.     **N/A
yarn.client.failover-sleep-max-ms**When HA is enabled, the maximum sleep time (in milliseconds)       between failovers. When set, this overrides the       yarn.resourcemanager.connect.* settings. When not set,       yarn.resourcemanager.connect.retry-interval.ms is used instead.**N/A
yarn.client.failover-retries**When HA is enabled, the number of retries per       attempt to connect to a ResourceManager. In other words,       it is the ipc.client.connect.max.retries to be used during       failover attempts**0
yarn.client.failover-retries-on-socket-timeouts**When HA is enabled, the number of retries per       attempt to connect to a ResourceManager on socket timeouts. In other       words, it is the ipc.client.connect.max.retries.on.timeouts to be used       during failover attempts**0
yarn.resourcemanager.max-completed-applications**The maximum number of completed applications RM keeps. **10000
yarn.resourcemanager.delayed.delegation-token.removal-interval-ms**Interval at which the delayed token removal thread runs**30000
yarn.resourcemanager.proxy-user-privileges.enabled**If true, ResourceManager will have proxy-user privileges.     Use case: In a secure cluster, YARN requires the user hdfs delegation-tokens to     do localization and log-aggregation on behalf of the user. If this is set to true,     ResourceManager is able to request new hdfs delegation tokens on behalf of     the user. This is needed by long-running-service, because the hdfs tokens     will eventually expire and YARN requires new valid tokens to do localization     and log-aggregation. Note that to enable this use case, the corresponding     HDFS NameNode has to configure ResourceManager as the proxy-user so that     ResourceManager can itself ask for new tokens on behalf of the user when     tokens are past their max-life-time.**false
yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs**Interval for the roll over for the master key used to generate         application tokens     **86400
yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs**Interval for the roll over for the master key used to generate         container tokens. It is expected to be much greater than         yarn.nm.liveness-monitor.expiry-interval-ms and         yarn.resourcemanager.rm.container-allocation.expiry-interval-ms. Otherwise the         behavior is undefined.     **86400
yarn.resourcemanager.nodemanagers.heartbeat-interval-ms**The heart-beat interval in milliseconds for every NodeManager in the cluster.**1000
yarn.resourcemanager.nodemanager.minimum.version**The minimum allowed version of a connecting nodemanager.  The valid values are       NONE (no version checking), EqualToRM (the nodemanager's version is equal to       or greater than the RM version), or a Version String.**NONE
yarn.resourcemanager.scheduler.monitor.enable**Enable a set of periodic monitors (specified in         yarn.resourcemanager.scheduler.monitor.policies) that affect the         scheduler.**false
yarn.resourcemanager.scheduler.monitor.policies**The list of SchedulingEditPolicy classes that interact with         the scheduler. A particular module may be incompatible with the         scheduler, other policies, or a configuration of either.**org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy
yarn.resourcemanager.configuration.provider-class**The class to use as the configuration provider.     If org.apache.hadoop.yarn.LocalConfigurationProvider is used,     the local configuration will be loaded.     If org.apache.hadoop.yarn.FileSystemBasedConfigurationProvider is used,     the configuration which will be loaded should be uploaded to remote File system first.     **org.apache.hadoop.yarn.LocalConfigurationProvider
yarn.resourcemanager.system-metrics-publisher.enabled**The setting that controls whether yarn system metrics is     published on the timeline server or not by RM.**false
yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size**Number of worker threads that send the yarn system metrics     data.**10
yarn.nodemanager.hostname**The hostname of the NM.**0.0.0.0
yarn.nodemanager.address**The address of the container manager in the NM.**${yarn.nodemanager.hostname}:0
yarn.nodemanager.bind-host**       The actual address the server will bind to. If this optional address is       set, the RPC and webapp servers will bind to this address and the port specified in       yarn.nodemanager.address and yarn.nodemanager.webapp.address, respectively. This is       most useful for making NM listen to all interfaces by setting to 0.0.0.0.     **N/A
yarn.nodemanager.admin-env**Environment variables that should be forwarded from the NodeManager's environment to the container's.**MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX
yarn.nodemanager.env-whitelist**Environment variables that containers may override rather than use NodeManager's default.**JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME
yarn.nodemanager.container-executor.class**who will execute(launch) the containers.**org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor
yarn.nodemanager.container-manager.thread-count**Number of threads container manager uses.**20
yarn.nodemanager.delete.thread-count**Number of threads used in cleanup.**4
yarn.nodemanager.delete.debug-delay-sec**       Number of seconds after an application finishes before the nodemanager's        DeletionService will delete the application's localized file directory       and log directory.              To diagnose Yarn application problems, set this property's value large       enough (for example, to 600 = 10 minutes) to permit examination of these       directories. After changing the property's value, you must restart the        nodemanager in order for it to have an effect.        The roots of Yarn applications' work directories is configurable with       the yarn.nodemanager.local-dirs property (see below), and the roots       of the Yarn applications' log directories is configurable with the        yarn.nodemanager.log-dirs property (see also below).     **0
yarn.nodemanager.keytab**Keytab for NM.**/etc/krb5.keytab
yarn.nodemanager.local-dirs**List of directories to store localized files in. An        application's localized file directory will be found in:       ${yarn.nodemanager.local-dirs}/usercache/${user}/appcache/application_${appid}.       Individual containers' work directories, called container_${contid}, will       be subdirectories of this.    **${hadoop.tmp.dir}/nm-local-dir
yarn.nodemanager.local-cache.max-files-per-directory**It limits the maximum number of files which will be localized       in a single local directory. If the limit is reached then sub-directories       will be created and new files will be localized in them. If it is set to       a value less than or equal to 36 [which are sub-directories (0-9 and then       a-z)] then NodeManager will fail to start. For example; [for public       cache] if this is configured with a value of 40 ( 4 files +       36 sub-directories) and the local-dir is "/tmp/local-dir1" then it will       allow 4 files to be created directly inside "/tmp/local-dir1/filecache".       For files that are localized further it will create a sub-directory "0"       inside "/tmp/local-dir1/filecache" and will localize files inside it       until it becomes full. If a file is removed from a sub-directory that       is marked full, then that sub-directory will be used back again to       localize files.    **8192
yarn.nodemanager.localizer.address**Address where the localizer IPC is.**${yarn.nodemanager.hostname}:8040
yarn.nodemanager.localizer.cache.cleanup.interval-ms**Interval in between cache cleanups.**600000
yarn.nodemanager.localizer.cache.target-size-mb**Target size of localizer cache in MB, per nodemanager. It is       a target retention size that only includes resources with PUBLIC and        PRIVATE visibility and excludes resources with APPLICATION visibility     **10240
yarn.nodemanager.localizer.client.thread-count**Number of threads to handle localization requests.**5
yarn.nodemanager.localizer.fetch.thread-count**Number of threads to use for localization fetching.**4
yarn.nodemanager.log-dirs**       Where to store container logs. An application's localized log directory        will be found in ${yarn.nodemanager.log-dirs}/application_${appid}.       Individual containers' log directories will be below this, in directories        named container_{$contid}. Each container directory will contain the files       stderr, stdin, and syslog generated by that container.     **${yarn.log.dir}/userlogs
yarn.log-aggregation-enable**Whether to enable log aggregation. Log aggregation collects       each container's logs and moves these logs onto a file-system, for e.g.       HDFS, after the application completes. Users can configure the       "yarn.nodemanager.remote-app-log-dir" and       "yarn.nodemanager.remote-app-log-dir-suffix" properties to determine       where these logs are moved to. Users can access the logs via the       Application Timeline Server.     **false
yarn.log-aggregation.retain-seconds**How long to keep aggregation logs before deleting them.  -1 disables.      Be careful set this too small and you will spam the name node.**-1
yarn.log-aggregation.retain-check-interval-seconds**How long to wait between aggregated log retention checks.     If set to 0 or a negative value then the value is computed as one-tenth     of the aggregated log retention time. Be careful set this too small and     you will spam the name node.**-1
yarn.nodemanager.log.retain-seconds**Time in seconds to retain user logs. Only applicable if     log aggregation is disabled     **10800
yarn.nodemanager.remote-app-log-dir**Where to aggregate logs to.**/tmp/logs
yarn.nodemanager.remote-app-log-dir-suffix**The remote log dir will be created at        {yarn.nodemanager.remote-app-log-dir}/${user}/{thisParam}     **logs
yarn.nodemanager.resource.memory-mb**Amount of physical memory, in MB, that can be allocated      for containers.**8192
yarn.nodemanager.pmem-check-enabled**Whether physical memory limits will be enforced for     containers.**true
yarn.nodemanager.vmem-check-enabled**Whether virtual memory limits will be enforced for     containers.**true
yarn.nodemanager.vmem-pmem-ratio**Ratio between virtual memory to physical memory when     setting memory limits for containers. Container allocations are     expressed in terms of physical memory, and virtual memory usage     is allowed to exceed this allocation by this ratio.     **2.1
yarn.nodemanager.resource.cpu-vcores**Number of vcores that can be allocated     for containers. This is used by the RM scheduler when allocating     resources for containers. This is not used to limit the number of     physical cores used by YARN containers.**8
yarn.nodemanager.resource.percentage-physical-cpu-limit**Percentage of CPU that can be allocated     for containers. This setting allows users to limit the amount of     CPU that YARN containers use. Currently functional only     on Linux using cgroups. The default is to use 100% of CPU.     **100
yarn.nodemanager.webapp.address**NM Webapp address.**${yarn.nodemanager.hostname}:8042
yarn.nodemanager.container-monitor.interval-ms**How often to monitor containers.**3000
yarn.nodemanager.container-monitor.resource-calculator.class**Class that calculates containers current resource utilization.**N/A
yarn.nodemanager.health-checker.interval-ms**Frequency of running node health script.**600000
yarn.nodemanager.health-checker.script.timeout-ms**Script time out period.**1200000
yarn.nodemanager.health-checker.script.path**The health check script to run.**N/A
yarn.nodemanager.health-checker.script.opts**The arguments to pass to the health check script.**N/A
yarn.nodemanager.disk-health-checker.interval-ms**Frequency of running disk health checker code.**120000
yarn.nodemanager.disk-health-checker.min-healthy-disks**The minimum fraction of number of disks to be healthy for the     nodemanager to launch new containers. This correspond to both     yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs. i.e. If there     are less number of healthy local-dirs (or log-dirs) available, then     new containers will not be launched on this node.**0.25
yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage**The maximum percentage of disk space utilization allowed after      which a disk is marked as bad. Values can range from 0.0 to 100.0.      If the value is greater than or equal to 100, the nodemanager will check      for full disk. This applies to yarn.nodemanager.local-dirs and     yarn.nodemanager.log-dirs.**90.0
yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb**The minimum space that must be available on a disk for     it to be used. This applies to yarn.nodemanager.local-dirs and     yarn.nodemanager.log-dirs.**0
yarn.nodemanager.linux-container-executor.path**The path to the Linux container executor.**N/A
yarn.nodemanager.linux-container-executor.resources-handler.class**The class which should help the LCE handle resources.**org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler
yarn.nodemanager.linux-container-executor.cgroups.hierarchy**The cgroups hierarchy under which to place YARN proccesses (cannot contain commas).     If yarn.nodemanager.linux-container-executor.cgroups.mount is false (that is, if cgroups have     been pre-configured), then this cgroups hierarchy must already exist and be writable by the     NodeManager user, otherwise the NodeManager may fail.     Only used when the LCE resources handler is set to the CgroupsLCEResourcesHandler.**/hadoop-yarn
yarn.nodemanager.linux-container-executor.cgroups.mount**Whether the LCE should attempt to mount cgroups if not found.     Only used when the LCE resources handler is set to the CgroupsLCEResourcesHandler.**false
yarn.nodemanager.linux-container-executor.cgroups.mount-path**Where the LCE should attempt to mount cgroups if not found. Common locations     include /sys/fs/cgroup and /cgroup; the default location can vary depending on the Linux     distribution in use. This path must exist before the NodeManager is launched.     Only used when the LCE resources handler is set to the CgroupsLCEResourcesHandler, and     yarn.nodemanager.linux-container-executor.cgroups.mount is true.**N/A
yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users**This determines which of the two modes that LCE should use on       a non-secure cluster.  If this value is set to true, then all containers       will be launched as the user specified in       yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user.  If       this value is set to false, then containers will run as the user who       submitted the application.**true
yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user**The UNIX user that containers will run as when       Linux-container-executor is used in nonsecure mode (a use case for this       is using cgroups) if the       yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users is       set to true.**nobody
yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern**The allowed pattern for UNIX user names enforced by     Linux-container-executor when used in nonsecure mode (use case for this     is using cgroups). The default value is taken from /usr/sbin/adduser**^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$
yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage**This flag determines whether apps should run with strict resource limits     or be allowed to consume spare resources if they need them. For example, turning the     flag on will restrict apps to use only their share of CPU, even if the node has spare     CPU cycles. The default value is false i.e. use available resources. Please note that     turning this flag on may reduce job throughput on the cluster.**false
yarn.nodemanager.windows-container.memory-limit.enabled**This flag determines whether memory limit will be set for the Windows Job     Object of the containers launched by the default container executor.**false
yarn.nodemanager.windows-container.cpu-limit.enabled**This flag determines whether CPU limit will be set for the Windows Job     Object of the containers launched by the default container executor.**false
yarn.nodemanager.log-aggregation.compression-type**T-file compression types used to compress aggregated logs.**none
yarn.nodemanager.principal**The kerberos principal for the node manager.**N/A
yarn.nodemanager.aux-services**A comma separated list of services where service name should only       contain a-zA-Z0-9_ and can not start with numbers**N/A
yarn.nodemanager.sleep-delay-before-sigkill.ms**No. of ms to wait between sending a SIGTERM and SIGKILL to a container**250
yarn.nodemanager.process-kill-wait.ms**Max time to wait for a process to come up when trying to cleanup a container**2000
yarn.nodemanager.resourcemanager.minimum.version**The minimum allowed version of a resourcemanager that a nodemanager will connect to.         The valid values are NONE (no version checking), EqualToNM (the resourcemanager's version is        equal to or greater than the NM version), or a Version String.**NONE
yarn.client.nodemanager-client-async.thread-pool-max-size**Max number of threads in NMClientAsync to process container     management events**500
yarn.client.nodemanager-connect.max-wait-ms**Max time to wait to establish a connection to NM**180000
yarn.client.nodemanager-connect.retry-interval-ms**Time interval between each attempt to connect to NM**10000
yarn.client.max-cached-nodemanagers-proxies**       Maximum number of proxy connections to cache for node managers. If set       to a value greater than zero then the cache is enabled and the NMClient       and MRAppMaster will cache the specified number of node manager proxies.       There will be at max one proxy per node manager. Ex. configuring it to a       value of 5 will make sure that client will at max have 5 proxies cached       with 5 different node managers. These connections for these proxies will       be timed out if idle for more than the system wide idle timeout period.       Note that this could cause issues on large clusters as many connections       could linger simultaneously and lead to a large number of connection       threads. The token used for authentication will be used only at       connection creation time. If a new token is received then the earlier       connection should be closed in order to use the new token. This and       (yarn.client.nodemanager-client-async.thread-pool-max-size) are related       and should be in sync (no need for them to be equal).       If the value of this property is zero then the connection cache is       disabled and connections will use a zero idle timeout to prevent too       many connection threads on large clusters.     **0
yarn.nodemanager.recovery.enabled**Enable the node manager to recover after starting**false
yarn.nodemanager.recovery.dir**The local filesystem directory in which the node manager will     store state when recovery is enabled.**${hadoop.tmp.dir}/yarn-nm-recovery
yarn.nodemanager.recovery.compaction-interval-secs**The time in seconds between full compactions of the NM state     database. Setting the interval to zero disables the full compaction     cycles.**3600
yarn.nodemanager.container-metrics.unregister-delay-ms**     The delay time ms to unregister container metrics after completion.     **10000
yarn.nodemanager.docker-container-executor.exec-name**       Name or path to the Docker client.     **/usr/bin/docker
yarn.nodemanager.aux-services.mapreduce_shuffle.class**N/A**org.apache.hadoop.mapred.ShuffleHandler
mapreduce.job.jar**N/A**N/A
mapreduce.job.hdfs-servers**N/A**${fs.defaultFS}
yarn.web-proxy.principal**The kerberos principal for the proxy, if the proxy is not     running as part of the RM.**N/A
yarn.web-proxy.keytab**Keytab for WebAppProxy, if the proxy is not running as part of      the RM.**N/A
yarn.web-proxy.address**The address for the web proxy as HOST:PORT, if this is not      given then the proxy will run as part of the RM**N/A
yarn.application.classpath**       CLASSPATH for YARN applications. A comma-separated list       of CLASSPATH entries. When this value is empty, the following default       CLASSPATH for YARN applications would be used.        For Linux:       $HADOOP_CONF_DIR,       $HADOOP_COMMON_HOME/share/hadoop/common/*,       $HADOOP_COMMON_HOME/share/hadoop/common/lib/*,       $HADOOP_HDFS_HOME/share/hadoop/hdfs/*,       $HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,       $HADOOP_YARN_HOME/share/hadoop/yarn/*,       $HADOOP_YARN_HOME/share/hadoop/yarn/lib/*       For Windows:       %HADOOP_CONF_DIR%,       %HADOOP_COMMON_HOME%/share/hadoop/common/*,       %HADOOP_COMMON_HOME%/share/hadoop/common/lib/*,       %HADOOP_HDFS_HOME%/share/hadoop/hdfs/*,       %HADOOP_HDFS_HOME%/share/hadoop/hdfs/lib/*,       %HADOOP_YARN_HOME%/share/hadoop/yarn/*,       %HADOOP_YARN_HOME%/share/hadoop/yarn/lib/*     **N/A
yarn.timeline-service.enabled**     In the server side it indicates whether timeline service is enabled or not.     And in the client side, users can enable it to indicate whether client wants     to use timeline service. If it's enabled in the client side along with     security, then yarn client tries to fetch the delegation tokens for the     timeline server.     **false
yarn.timeline-service.hostname**The hostname of the timeline service web application.**0.0.0.0
yarn.timeline-service.address**This is default address for the timeline server to start the     RPC server.**${yarn.timeline-service.hostname}:10200
yarn.timeline-service.webapp.address**The http address of the timeline service web application.**${yarn.timeline-service.hostname}:8188
yarn.timeline-service.webapp.https.address**The https address of the timeline service web application.**${yarn.timeline-service.hostname}:8190
yarn.timeline-service.bind-host**       The actual address the server will bind to. If this optional address is       set, the RPC and webapp servers will bind to this address and the port specified in       yarn.timeline-service.address and yarn.timeline-service.webapp.address, respectively.       This is most useful for making the service listen to all interfaces by setting to       0.0.0.0.     **N/A
yarn.timeline-service.generic-application-history.max-applications**       Defines the max number of applications could be fetched using REST API or       application history protocol and shown in timeline server web ui.     **10000
yarn.timeline-service.store-class**Store class name for timeline store.**org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore
yarn.timeline-service.ttl-enable**Enable age off of timeline store data.**true
yarn.timeline-service.ttl-ms**Time to live for timeline store data in milliseconds.**604800000
yarn.timeline-service.leveldb-timeline-store.path**Store file name for leveldb timeline store.**${hadoop.tmp.dir}/yarn/timeline
yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms**Length of time to wait between deletion cycles of leveldb timeline store in milliseconds.**300000
yarn.timeline-service.leveldb-timeline-store.read-cache-size**Size of read cache for uncompressed blocks for leveldb timeline store in bytes.**104857600
yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size**Size of cache for recently read entity start times for leveldb timeline store in number of entities.**10000
yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size**Size of cache for recently written entity start times for leveldb timeline store in number of entities.**10000
yarn.timeline-service.handler-thread-count**Handler thread count to serve the client RPC requests.**10
yarn.timeline-service.http-authentication.type**       Defines authentication used for the timeline server HTTP endpoint.       Supported values are: simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME#     **simple
yarn.timeline-service.http-authentication.simple.anonymous.allowed**       Indicates if anonymous requests are allowed by the timeline server when using       'simple' authentication.     **true
yarn.timeline-service.principal**The Kerberos principal for the timeline server.**N/A
yarn.timeline-service.keytab**The Kerberos keytab for the timeline server.**/etc/krb5.keytab
yarn.timeline-service.ui-names**Comma separated list of UIs that will be hosted**N/A
yarn.timeline-service.client.max-retries**     Default maximum number of retires for timeline servive client     and value -1 means no limit.     **30
yarn.timeline-service.client.best-effort**Client policy for whether timeline operations are non-fatal.     Should the failure to obtain a delegation token be considered an application     failure (option = false),  or should the client attempt to continue to     publish information without it (option=true)**false
yarn.timeline-service.client.retry-interval-ms**     Default retry time interval for timeline servive client.     **1000
yarn.timeline-service.recovery.enabled**Enable timeline server to recover state after starting. If     true, then yarn.timeline-service.state-store-class must be specified.     **false
yarn.timeline-service.state-store-class**Store class name for timeline state store.**org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore
yarn.timeline-service.leveldb-state-store.path**Store file name for leveldb state store.**${hadoop.tmp.dir}/yarn/timeline
yarn.sharedcache.enabled**Whether the shared cache is enabled**false
yarn.sharedcache.root-dir**The root directory for the shared cache**/sharedcache
yarn.sharedcache.nested-level**The level of nested directories before getting to the checksum     directories. It must be non-negative.**3
yarn.sharedcache.store.class**The implementation to be used for the SCM store**org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore
yarn.sharedcache.app-checker.class**The implementation to be used for the SCM app-checker**org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker
yarn.sharedcache.store.in-memory.staleness-period-mins**A resource in the in-memory store is considered stale     if the time since the last reference exceeds the staleness period.     This value is specified in minutes.**10080
yarn.sharedcache.store.in-memory.initial-delay-mins**Initial delay before the in-memory store runs its first check     to remove dead initial applications. Specified in minutes.**10
yarn.sharedcache.store.in-memory.check-period-mins**The frequency at which the in-memory store checks to remove     dead initial applications. Specified in minutes.**720
yarn.sharedcache.admin.address**The address of the admin interface in the SCM (shared cache manager)**0.0.0.0:8047
yarn.sharedcache.admin.thread-count**The number of threads used to handle SCM admin interface (1 by default)**1
yarn.sharedcache.webapp.address**The address of the web application in the SCM (shared cache manager)**0.0.0.0:8788
yarn.sharedcache.cleaner.period-mins**The frequency at which a cleaner task runs.     Specified in minutes.**1440
yarn.sharedcache.cleaner.initial-delay-mins**Initial delay before the first cleaner task is scheduled.     Specified in minutes.**10
yarn.sharedcache.cleaner.resource-sleep-ms**The time to sleep between processing each shared cache     resource. Specified in milliseconds.**0
yarn.sharedcache.uploader.server.address**The address of the node manager interface in the SCM     (shared cache manager)**0.0.0.0:8046
yarn.sharedcache.uploader.server.thread-count**The number of threads used to handle shared cache manager     requests from the node manager (50 by default)**50
yarn.sharedcache.client-server.address**The address of the client interface in the SCM     (shared cache manager)**0.0.0.0:8045
yarn.sharedcache.client-server.thread-count**The number of threads used to handle shared cache manager     requests from clients (50 by default)**50
yarn.sharedcache.checksum.algo.impl**The algorithm used to compute checksums of files (SHA-256 by     default)**org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl
yarn.sharedcache.nm.uploader.replication.factor**The replication factor for the node manager uploader for the     shared cache (10 by default)**10
yarn.sharedcache.nm.uploader.thread-count**The number of threads used to upload files from a node manager     instance (20 by default)**20
yarn.client.application-client-protocol.poll-interval-ms**The interval that the yarn client library uses to poll the     completion status of the asynchronous API of application client protocol.     **200
yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled**RSS usage of a process computed via      /proc/pid/stat is not very accurate as it includes shared pages of a     process. /proc/pid/smaps provides useful information like     Private_Dirty, Private_Clean, Shared_Dirty, Shared_Clean which can be used     for computing more accurate RSS. When this flag is enabled, RSS is computed     as Min(Shared_Dirty, Pss) + Private_Clean + Private_Dirty. It excludes     read-only shared mappings in RSS computation.       **false
yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds**Defines how often NMs wake up to upload log files.     The default value is -1. By default, the logs will be uploaded when     the application is finished. By setting this configure, logs can be uploaded     periodically when the application is running. The minimum rolling-interval-seconds     can be set is 3600.     **-1
yarn.nodemanager.webapp.cross-origin.enabled**Flag to enable cross-origin (CORS) support in the NM. This flag     requires the CORS filter initializer to be added to the filter initializers     list in core-site.xml.**false
